{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 377 Fall 2018\n",
    "\n",
    "#### Name:\n",
    "#### Section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation Statement:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Predicting Spam\n",
    "### Data Collection, Summarization, Inference and Prediction\n",
    "\n",
    "<img style=\"float: right;\" src=\"img\\spam3.jpg\">\n",
    "This project is designed to cover many of the main ideas of the entire course. Ultimately we want to predict if an email is spam. In the course of doing this, we will collect data, clean it up, work with string data, make a simple inference, and then build a naive bayes model from the ground up. \n",
    "\n",
    "\n",
    "By the end of project, you should know how to:\n",
    "\n",
    "1. Find and import data.\n",
    "2. Use regular expressions to edit string data.\n",
    "3. Determine if a word helps to identify an email as spam or not.\n",
    "4. Create a function to predict the type of email using the ideas of Bayesian Classification.\n",
    "5. Assess your model and propose improvements.\n",
    "\n",
    "**Advice.** Develop your answers incrementally. To perform a complicated table manipulation, break it up into steps, perform each step on a different line, give a new name to each result, and check that each intermediate result is what you expect by displaying it. You can add additional names or functions to the provided cells in order to organize your work. \n",
    "\n",
    "**Authorized Resources:** Anyone and anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Notebook Setup  \n",
    "\n",
    "Create a folder on your local drive and place this notebook in it. Create a subfolder called `data` copy the file `subject_lines.csv` into it from our Google drive. Create another subfolder called `img` and copy `spam3.jpg` into it, this file can also be found on the Google drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Background Information \n",
    "\n",
    "There are a couple of reference papers that may be of interest to explore. The first is \"Better Bayesian Filtering\" by Paul Graham,http://bit.ly/1ycPbiy. The second is \"A Plan for Spam\" also by Paul Graham, http://bit.ly/1ycPcmA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Packages  \n",
    "\n",
    "To get started, load `datascience`, `numpy`, `mathplotlib.pyplot`, `math`, `re`, and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this line as written\n",
    "import datascience as ds #note the labs use from datascience import * but this is not a good practice\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get Data\n",
    "\n",
    "We are going to use data from the [Apache SpamAssasian](https://spamassassin.apache.org/) website. In particular we want data from their public corpus; see the readme document at https://spamassassin.apache.org/old/publiccorpus/.\n",
    "\n",
    "We have provided you with a csv file that contains two columns. The first is the subject line and the second is whether the email is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data. It can be found on the Google drive with the name `subject_lines.csv`. Don't forget that you have imported the `datascience` package under the alias `ds`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code to read in the data\n",
    "Spam_data_table = ...\n",
    "Spam_data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a count of the emails. Use the group command to get a count of spam and ham emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code\n",
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering  \n",
    "\n",
    "We are dealing with string data as our predictor. We first need to clean it up. The choices we make here will potentially have a big impact on the quality of the model. Ideally we would go back and test the sensitivity of our results to these choices.\n",
    "\n",
    "First we will make all the text lower case. This will ensure that words such as Free and free are viewed as equivalent. This may not be a good idea for spam detection as a word in all capital letters might be more indicative of spam.  \n",
    "\n",
    "We will be dealing with frequencies in this project but we could also try other ideas such as vector encoding and $n$-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data\n",
    "\n",
    "We will first get the subject lines in a standard format with lower cases, no punctuation, and the removal of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the function we need. Execute this line\n",
    "'FREE'.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to tokenize our string. This means to split the string into a list of words. This requires the use of regular expressions, https://docs.python.org/2/library/re.html. \n",
    "\n",
    "We will also need to remove stop words. Stop words are common words such as `and` or `we`, that add little predictive ability. We will load the natural langauge tool kit package, `nltk`, to get a list of stop words. If you are having trouble loading the `nltk` package we also provide the stop words as a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using nltk package, the preferred method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the following commands\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Getting the English stop words from nltk\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "print(stop_words[:8])\n",
    "\n",
    "#We will convert our stopwords to a set as set lookup is much faster\n",
    "\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a csv file, the backup method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(pd.read_csv('stop_words.csv')[\"stopwords\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip off unwanted text from subject line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this line as is\n",
    "# Using data science table to demenostrate how to make lower case and keep only words and numbers\n",
    "print(Spam_data_table.take(np.arange(5)))\n",
    "print(\"\\n\")\n",
    "for subject in Spam_data_table.take(np.arange(5)).column(0):\n",
    "    print(re.findall(\"[a-z0-9']+\",subject.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the stop words and split the string into words. We will run some test code on the first 5 lines for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing code, run this line of code\n",
    "for subject in Spam_data_table.take(np.arange(5)).column(0):\n",
    "    final_words = []\n",
    "    print(\"Original subject line: \",subject)\n",
    "    print(\"Split line: \",re.findall(\"[a-z0-9']+\",subject.lower()))\n",
    "    words = re.findall(\"[a-z0-9']+\",subject.lower())\n",
    "    [final_words.append(word) for word in words if word not in stop_words]\n",
    "    print(\"Clean line: \",final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subject Line Frequency  \n",
    "\n",
    "Before going further with inference or prediction, let's summarize the data. We want to count the total number of times a word occurrs in a subject line separately in both the spam and ham data sets. We need to count the word once per subject line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to tokenize each subject line. This means reducing to lower case, removing stop words, and then splits into individual words. Use the code in the previous line as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this function\n",
    "def token(subject,sw=stop_words):\n",
    "    words = re.findall(...)\n",
    "    return set([word for word in ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's run some code on a subset, the first 3 lines, of the data. We need to get the subject line column, tokenize it, and then flatten into a list instead of a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code\n",
    "# Get the first 5 subject lines for spam\n",
    "spam_subjects = Spam_data_table.where('Spam',True).take(np.arange(5)).column(0)\n",
    "#Tokenize and then append\n",
    "spam_words=[]\n",
    "for subject in spam_subjects[:3]:\n",
    "    spam_words.append(token(subject,stop_words))\n",
    "print(\"List after using token: \",spam_words,\"\\n\")\n",
    "#Create one long list of words\n",
    "spam_words = [item for sublist in spam_words for item in sublist]\n",
    "print(\"The flattened list: \",spam_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created code to generate a list of spam words taken from the first 3 subject lines, copy and paste the above snippet and modify it below to go through ALL spam. Note that you should no longer print the lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the subject lines for spam\n",
    "...\n",
    "print(\"There are\",len(spam_words),\"tokenized words in the spam subject lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize multiple subjects with the apply function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code\n",
    "# Using apply function in datascience\n",
    "Spam_data_table.take(np.arange(5)).with_columns(\"Subject\",Spam_data_table.take(np.arange(5)).apply(token,\"Subject Line\")).\\\n",
    "drop(0).select(\"Subject\",\"Spam\").relabel(\"Subject\",\"Subject Line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the snippet above as an example, tokenize all subjects from spam e-mails only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the entire table\n",
    "Spam_data_table_spamonly = ...\n",
    "Spam_data_table_token = ...\n",
    "Spam_data_table_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tokenized all of the spam e-mails, get the frequency of each of the words that appears in spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts and frequency\n",
    "# Flatten first\n",
    "spam_words = [item for sublist in Spam_data_table_token.column(0) for item in sublist]\n",
    "# Put into a table\n",
    "spam_table = ...\n",
    "spam_word_counts = ...\n",
    "spam_word_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\",spam_word_counts.num_rows,\"unique words in the tokenized spam subject lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create a column that has the subject line frequency of each word. This is the percentage of subject lines in the spam emails that contain the word of interest. We also sort the table from most frequent to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell of code\n",
    "spam_word_counts = spam_word_counts.with_column('Spam Subject Line Frequency',spam_word_counts.column('Spam Count')/500).\\\n",
    "set_format(\"Spam Subject Line Frequency\", ds.NumberFormatter(decimals=4))\n",
    "spam_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, 9.2% of the subject lines in the spam emails contain the word `ilug`. As a reminder, the vocabulary of words in the spam subject line consists of 2408 words where 1135 are unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_word_counts.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total of the spam word count column in `spam_word_counts`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_spam_word_counts = ...\n",
    "sum_spam_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the words only occur once. Let's look at some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table of words where the word only occurs once. There should be three columns, word, spam count and frequency.\n",
    "spam_word_one = ...\n",
    "spam_word_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the results so far, we will use a bar chart. Create a bar chart showing the frequency of the 15 most frequent words that appear in spam subject lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a horizontal bar chart\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ham e-mails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this work for the ham e-mails. First, tokenize the ham subject lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ham_data_table_hamonly = ...\n",
    "Ham_data_table_token = ...\n",
    "Ham_data_table_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word counts for words that appeared in ham subject lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts and frequency\n",
    "# Flatten first\n",
    "ham_words = [item for sublist in Ham_data_table_token.column(0) for item in sublist]\n",
    "# Put into a table\n",
    "ham_table = ...\n",
    "ham_word_counts = ...\n",
    "ham_word_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\",ham_word_counts.num_rows,\"unique words in the tokenized ham subject lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ham_word_counts = ...\n",
    "print(\"There are\",sum_ham_word_counts,\"tokenized words in the ham subject lines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word_counts = ham_word_counts.with_column('Ham Subject Line Frequency',ham_word_counts.column('Ham Count')/2800).\\\n",
    "set_format(\"Ham Subject Line Frequency\", ds.NumberFormatter(decimals=4))\n",
    "ham_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a barplot showing the frequency of the 15 most frequent words that appear in ham subject lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 12163 words in the ham email subject lines of which 3727 are unique. Which words appear only once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word_one = ...\n",
    "ham_word_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, in the spam subject line there are 1135 unique words while there are 3727 in the ham. When we merge these data sets there will only be partial overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_word_counts.num_rows #Number of unique words in spam subject lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_word_counts.num_rows #Number of unique words in ham subject lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more words in the ham subject lines than in the spam subject lines. We need to get one table with the words and counts. This will increase the number of rows for the spam. The built-in join function from the `datascience` package only performs an inner joining. We need to perform an outer join. We cannot determine how to do this with the join function from `datascience` so let's convert the table to pandas dataframes and then perform an outer join, see https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/. By default, pandas repalces missing values with NaN, not a number. We must replace those missing values with 0. Finally, we can convert back to a datascience table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "# Convert data.science table to pandas dataframe\n",
    "word_counts = pd.merge(ham_word_counts.to_df(),spam_word_counts.to_df(),how=\"outer\").fillna(0)\n",
    "word_counts = ds.Table.from_df(word_counts)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results by looking at the most common words in the ham subject lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "word_counts.take(np.arange(10)).select('Words','Ham Subject Line Frequency','Spam Subject Line Frequency').barh('Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the same visualization but with the most common words in the spam emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about words that appear in spam and ham e-mail subject lines? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n",
    "http://localhost:8888/notebooks/Documents/Classes/Books/Stats/Python%20Data%20Science%20Handbook/PythonDataScienceHandbook-master/notebooks/05.05-Naive-Bayes.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
